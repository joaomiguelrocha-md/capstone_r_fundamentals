---
title: "Modelos estatísticos"
format: html
editor: visual
---

## Modelos

-   If a systematic relationship exists between two variables it will appear as a pattern in the data. If you spot a pattern, ask yourself:

    -   Could this pattern be due to coincidence (i.e. random chance)?

    -   How can you describe the relationship implied by the pattern?

    -   How strong is the relationship implied by the pattern?

    -   What other variables might affect the relationship?

    -   Does the relationship change if you look at individual subgroups of the data?

```{r}

gdh_uls |> 
  ggplot(aes(x = idade, y = dias_int)) + 
  geom_point(alpha = 0.5) +
  coord_cartesian(xlim = c(65, 109), ylim = c(0,10))


gdh_model <- gdh_uls |> 
  mutate(
    sexo_f = factor(sexo)
  ) |> 
  filter(
    dias_int < 50
  )
  

gdh_model |>  t_test(idade ~ sexo_f)


model1 <- lm(dias_int ~ idade, data = gdh_model)

summary(model1)

model1_tidy <- tidy(model1, conf.int = TRUE)
model1_tidy


model2 <- lm(dias_int ~ idade, data = gdh_model)

summary(model2)


gdh_uls |> 
  ggplot(aes(x = idade, y = dias_int, color = sexo_f)) +  # Definir 'age' e 'resting_bp' como variáveis nos eixos x e y, respectivamente, e 'sex' para a cor dos pontos
  geom_point(alpha = 0.7)

model3 <- lm(dias_int ~ idade + sexo_f, data = gdh_uls)
model3_tidy <- tidy(model3, conf.int = TRUE)
model3_tidy

check_model(model3)



```

```{r}
gdh_rf <-  gdh_base_clean |> 
 mutate(int_prolongado = ifelse(dias_int > median(dias_int), 1, 0),
        int_prolongado = factor(int_prolongado),
        sexo = factor(sexo),
        cod_conc = factor(cod_conc))



pca_rec <- recipe(~ ., data = gdh_rf) %>%
  step_rm(label) %>%
  step_normalize(all_predictors())

pca_wf <- workflow() %>%
  add_recipe(pca_rec)

pca_complete %>%
  extract_fit_engine() %>%
  fviz_dend(main = "Complete")

#  adm_tip,
    # tipo_port_apr31, 
    # dsp, 
    # dt_mun,



```

```{r}
# Load libraries
library(tidymodels)
library(dplyr)

# Assuming your dataframe is called df and has columns: sex, age, city, duration_of_stay

# 1. Prepare the binary target and factor variables

gdh_ml <-  gdh_base_clean |> 
  select(
    dias_int,
    idade,
    sexo,
    cod_conc_chr,
    adm_tip
  ) |> 
  filter(
    dias_int > 0,
    idade > 18
  ) |> 
  mutate(
   int_prolongado = ifelse(dias_int > mean(dias_int, na.rm = TRUE), "prolongado", "curto"),
   int_prolongado = factor(int_prolongado),
   sexo = factor(sexo)
   ) 
 

# 2. Split data into training and testing sets (stratified)
set.seed(123)
data_split <- initial_split(gdh_ml, prop = 0.7, strata = int_prolongado)
train_data <- training(data_split)
test_data <- testing(data_split)

k_folds <- vfold_cv(train_data, v = 10, strata = int_prolongado)
# 3. Define recipe for pre-processing
data_recipe <- recipe(int_prolongado ~ idade + sexo + cod_conc_chr, data = train_data) %>%
  step_dummy(all_nominal(), -all_outcomes()) |>    # Convert sex, city to dummies
  step_normalize(all_numeric(), -all_outcomes())  # Remove zero-variance predictors if any

# 4. Specify logistic regression model
log_spec <- logistic_reg()  |> 
  set_engine("glm") |> 
  set_mode("classification")

# 5. Logistic regression workflow
log_wflow <- workflow()  |> 
  add_recipe(data_recipe)  |> 
  add_model(log_spec)

# 6. Fit logistic regression model
log_fit <- fit(log_wflow, data = train_data)

# 7. Predict and evaluate logistic regression model on test set
logistic_preds <- predict(log_fit, test_data, type = "prob") %>%
  bind_cols(predict(log_fit, test_data)) %>%
  bind_cols(test_data %>% select(int_prolongado))

# Evaluate performance
logistic_roc_auc <- roc_auc(logistic_preds, truth = int_prolongado, .pred_prolongado)
logistic_accuracy <- accuracy(logistic_preds, truth = int_prolongado, .pred_class)

logistic_accuracy
# 8. Random Forest model specification
rf_spec <- rand_forest(trees = 20) %>%
  set_engine("ranger") %>%
  set_mode("classification")

# 9. Random Forest workflow
rf_wf <- workflow() %>%
  add_recipe(data_recipe) %>%
  add_model(rf_spec)

# 10. Fit random forest model
rf_fit <- fit(rf_wf, data = train_data)

# 11. Predict and evaluate random forest model on test set
rf_preds <- predict(rf_fit, test_data, type = "prob") %>%
  bind_cols(predict(rf_fit, test_data)) %>%
  bind_cols(test_data %>% select(int_prolongado))

rf_roc_auc <- roc_auc(rf_preds, truth = int_prolongado, .pred_prolongado)
rf_accuracy <- accuracy(rf_preds, truth = int_prolongado, .pred_class)


# 12. Print evaluation results
logistic_roc_auc
logistic_accuracy
rf_roc_auc
rf_accuracy




```

```{r}
tree_spec <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

# 5. Create workflow
tree_workflow <- workflow() %>%
  add_recipe(data_recipe) %>%
  add_model(tree_spec)

# 6. Fit the model on training data
tree_fit <- fit(tree_workflow, data = train_data)

# 7. Predict on test data
tree_preds <- predict(tree_fit, test_data, type = "prob") %>%
  bind_cols(predict(tree_fit, test_data)) %>%
  bind_cols(test_data %>% select(int_prolongado))

# 8. Evaluate performance
tree_roc_auc <- roc_auc(tree_preds, truth = int_prolongado, .pred_prolongado)  # change '.pred_long' if needed
tree_accuracy <- accuracy(tree_preds, truth = int_prolongado, .pred_class)

# 9. Print evaluation metrics
print(tree_roc_auc)
print(tree_accuracy)

# Optional: Plot the decision tree
library(rpart.plot)
rpart_model <- tree_fit %>% extract_fit_parsnip()
rpart.plot(rpart_model$fit)
```

```{r}
k_clusters <- 3

set.seed(123)
kmeans_result <- kmeansclust(gdh_ml, k = k_clusters, n_start = 25)

kmean
# 3. Add cluster assignments to original data
gdh_ml <- gdh_ml %>%
  mutate(cluster = as.factor(kmeans_result$cluster))

# 4. Visualize clusters using factoextra (PCA-based)
fviz_cluster(kmeans_result, data = gdh_ml, geom = "point") +
  labs(title = "K-means Clustering of Patients") +
  theme_minimal()
```

```{r}

gdh_ml_rec <- recipe(~ ., data = nci60) %>%
  step_rm(label) %>%
  step_normalize(all_predictors())

nci60_wf <- workflow() %>%
  add_recipe(nci60_rec)
```
